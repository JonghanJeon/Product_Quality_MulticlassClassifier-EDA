{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763ed0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46cee83",
   "metadata": {},
   "source": [
    "# Funtion Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55b8ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveEmptyColumn(train_X, test):\n",
    "    train_X = train_X.dropna(axis=1, how='all')\n",
    "    test = test[train_X.columns]\n",
    "\n",
    "    return train_X, test\n",
    "\n",
    "def DropDuplicateColumns(train_X, test):\n",
    "    train_X = train_X.loc[:,~train_X.T.duplicated(keep='first')]\n",
    "    test = test[train_X.columns]\n",
    "\n",
    "    return train_X, test\n",
    "\n",
    "def RemoveOneValueColumn(train_X, test):\n",
    "    for col in [x for x in train_X.columns if 'X_' in x]:\n",
    "        if len(train_X[col].value_counts())==1:\n",
    "            train_X = train_X.drop(col, axis=1)\n",
    "        \n",
    "    test = test[train_X.columns]\n",
    "\n",
    "    return train_X, test\n",
    "\n",
    "def ConcatProdLine(train_X, test):\n",
    "    train_X['PROD_LINE'] = train_X['PRODUCT_CODE']+'_'+train_X['LINE']\n",
    "    train_X = train_X.drop(['PRODUCT_CODE','LINE'],axis=1)\n",
    "    test['PROD_LINE'] = test['PRODUCT_CODE']+'_'+test['LINE']\n",
    "    test = test.drop(['PRODUCT_CODE','LINE'],axis=1)\n",
    "\n",
    "    return train_X, test\n",
    "\n",
    "def fillNa(train_X, test):\n",
    "    train_X = train_X.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "    \n",
    "    return train_X, test\n",
    "\n",
    "def DatascalingRobust(train_X, test):\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    num_features_train = [x for x in train_X.columns if \"X\" in x]\n",
    "    #train_x.select_dtypes(exclude=['object']).columns.to_list()\n",
    "\n",
    "    train_X[num_features_train] = scaler.fit_transform(train_X[num_features_train])\n",
    "    test[num_features_train] = scaler.transform(test[num_features_train])\n",
    "\n",
    "    return train_X, test\n",
    "\n",
    "def OnehotEncoder(train_X, test):\n",
    "    dummies_col = []\n",
    "    for c in ['PROD_LINE']:\n",
    "        df = pd.get_dummies(train_X[c])\n",
    "        train_X[df.columns] = df\n",
    "        train_X = train_X.drop(c, axis=1)\n",
    "        df = pd.get_dummies(test[c])\n",
    "        test[df.columns] = df\n",
    "        test = test.drop(c, axis=1)\n",
    "        dummies_col.extend(df.columns)\n",
    "        \n",
    "    return train_X, test, dummies_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f141f6",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aec3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train_X, train_y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        \n",
    "        self.train_X = torch.tensor(train_X.values, dtype=torch.float64)\n",
    "        self.train_y = torch.tensor(train_y.values, dtype=torch.float64)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.train_X[idx]\n",
    "        y = self.train_y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85d8f2",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee211596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, mode):\n",
    "    train_X = train_df.drop(columns=['Y_Quality', 'Y_Class', 'TIMESTAMP', 'PRODUCT_ID'])\n",
    "    if mode == 'single':\n",
    "        train_y = train_df['Y_Class']\n",
    "    if mode == 'dual':\n",
    "        train_y = train_df[['Y_Class', 'Y_Quality']]\n",
    "        \n",
    "    return train_X, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25ff12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/mooha/Desktop/LG Aimers/open/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/mooha/Desktop/LG Aimers/open/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5cfe5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = load_data(train_df, 'dual')\n",
    "test = test_df.drop(columns=['TIMESTAMP', 'PRODUCT_ID'])\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42, stratify=train_df['Y_Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc13c3c",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f8ae7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선은 test 대신에 val_X 로 대체 \n",
    "train_X, val_X = RemoveEmptyColumn(train_X, val_X)\n",
    "train_X, val_X = DropDuplicateColumns(train_X, val_X)\n",
    "train_X, val_X = RemoveOneValueColumn(train_X, val_X)\n",
    "train_X, val_X = ConcatProdLine(train_X, val_X)\n",
    "train_X, val_X = fillNa(train_X, val_X)\n",
    "train_X, val_X = DatascalingRobust(train_X, val_X)\n",
    "train_X, val_X, prod_dum = OnehotEncoder(train_X, val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9a3ad",
   "metadata": {},
   "source": [
    "위 데이터 전처리시 train_X 와 val_X 의 number of columns 가 다름. PROD_LINE 에서 val_X 가 하나 부족."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 8, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_X, val_y)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec410df",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6140f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1626, 8)\n",
    "        self.Classification = nn.Linear(8, 1)\n",
    "        self.Regression = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        y1= self.Classification(x)\n",
    "        y2= self.Regression(x)\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88cf50",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac100bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "EPOCHS = 1000\n",
    "best_acc = - np.inf   # init to negative infinity\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    loss_fn1 = nn.CrossEntropyLoss().to(device) # 다중클래스분류 손실 함수\n",
    "    loss_fn2 = mean_squared_error().to(device) # 회귀 손실 함수\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, EPOCHS):\n",
    "        train_loss = []\n",
    "        for x, y1, y2 in tqdm(iter(train_loader)):\n",
    "            x = x.to(device)\n",
    "            y1 = y1.to(device)\n",
    "            y2 = y2.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()   # clear gradients \n",
    "            pre_class, pre_regression = model(x)\n",
    "            \n",
    "            loss = loss_fn1(pre_class, y1)\n",
    "            loss2 = loss_fn2(pre_regression, y2)\n",
    "            loss_total = loss + loss2\n",
    "            \n",
    "            loss_total.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss_total.item())\n",
    "            \n",
    "        _val_loss, _val_acc = validation(model, loss_fn1, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val ACC : [{_val_acc:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_acc)\n",
    "            \n",
    "        if best_val_acc < _val_acc:\n",
    "            best_val_acc = _val_acc\n",
    "            best_model = model\n",
    "        \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e868b",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b369634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, ClassCriterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    for x, y1, y2 in tqdm(iter(val_loader)):\n",
    "        x = x.to(device)\n",
    "        y1 = y1.to(device)\n",
    "        y2 = y2.to(device)\n",
    "        \n",
    "        probs,_ = model(x)\n",
    "        \n",
    "        loss = ClassCriterion(probs, y1)\n",
    "        \n",
    "        probs  = probs.cpu().detach().numpy()\n",
    "        y1 = y1.cpu().detach().numpy()\n",
    "        \n",
    "        preds = probs > 0.5\n",
    "        \n",
    "        batch_acc = (y1 == preds).mean()\n",
    "        \n",
    "        val_acc.append(batch_acc)\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "    _val_loss = np.mean(val_loss)\n",
    "    _val_acc = np.mean(val_acc)\n",
    "    \n",
    "    return _val_loss, _val_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon_test",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
